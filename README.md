# Natural-Language-Processing-with-Attention-Models

- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019) https://arxiv.org/abs/1910.10683
- Reformer: The Efficient Transformer (Kitaev et al, 2020) https://arxiv.org/abs/2001.04451
- Attention Is All You Need (Vaswani et al, 2017) https://arxiv.org/abs/1706.03762
- Deep contextualized word representations (Peters et al, 2018) https://arxiv.org/pdf/1802.05365.pdf
- The Illustrated Transformer (Alammar, 2018) http://jalammar.github.io/illustrated-transformer/
- The Illustrated GPT-2 (Visualizing Transformer Language Models) (Alammar, 2019) http://jalammar.github.io/illustrated-gpt2/
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018) https://arxiv.org/abs/1810.04805
- How GPT3 Works - Visualizations and Animations (Alammar, 2020) http://jalammar.github.io/how-gpt3-works-visualizations-animations/
